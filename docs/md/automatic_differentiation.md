# Автоматическое дифференцирование (Automatic Differentiation, AD)

В математике и компьютерной алгебре автоматическое дифференцирование, также известное как алгоритмическое дифференцирование, вычислительное дифференцирование или арифметическое дифференцирование, представляет собой набор методов, предназначенных для вычисления частной производной функции, заданной компьютерной программой.

Автоматическое дифференцирование — это сложный и важный инструмент, который позволяет автоматизировать процесс одновременного вычисления как числовых значений сложных функций, так и их производных. При этом нет необходимости в символическом представлении производной — требуется только функциональное правило или алгоритм его вычисления.

Таким образом, автоматическое дифференцирование не является ни чисто числовым, ни чисто символическим методом, а представляет собой нечто среднее между ними. Оно также превосходит обычные численные методы: в отличие от традиционных методов, основанных на конечных разностях, автоматическое дифференцирование обладает "теоретической" точностью, а по сравнению с символьными алгоритмами требует меньших вычислительных затрат.

Автоматическое дифференцирование основано на том факте, что каждое компьютерное вычисление, независимо от его сложности, состоит из последовательности элементарных арифметических операций, таких как сложение, вычитание, умножение и деление, а также элементарных функций, таких как экспонента, логарифм, синус и косинус. При многократном применении правила цепочки к этим операциям частные производные произвольного порядка могут быть вычислены автоматически с рабочей точностью, используя при этом не больше небольшого постоянного множителя арифметических операций, чем в исходной программе.

## Основные принципы AD

1. **Разложение на элементарные операции**: Любая сложная функция разбивается на последовательность элементарных операций (сложение, умножение, тригонометрические функции и т.д.), для которых известны правила дифференцирования.

2. **Правило цепи (chain rule)**: Производная сложной функции вычисляется как произведение производных всех элементарных операций в порядке их выполнения.

3. **Два основных режима**:
   - Прямой режим (forward mode)
   - Обратный режим (reverse mode)

## Примеры реализации в Python

### 1. Использование библиотеки `autograd`

```python
import autograd.numpy as np
from autograd import grad

# Определяем функцию
def f(x):
    return x**2 + 3*x + 5

# Создаем функцию для вычисления производной
df_dx = grad(f)

# Вычисляем значение функции и производной в точке x=2.0
x = 2.0
print(f"f({x}) = {f(x)}")  # f(2.0) = 15.0
print(f"f'({x}) = {df_dx(x)}")  # f'(2.0) = 7.0
```

### 2. Использование PyTorch

```python
import torch

# Создаем тензор с require_grad=True для отслеживания операций
x = torch.tensor(2.0, requires_grad=True)

# Определяем функцию
y = x**2 + 3*x + 5

# Вычисляем градиент
y.backward()

print(f"f({x.item()}) = {y.item()}")  # f(2.0) = 15.0
print(f"f'({x.item()}) = {x.grad.item()}")  # f'(2.0) = 7.0
```

### 3. Использование TensorFlow

```python
import tensorflow as tf

# Создаем переменную
x = tf.Variable(2.0)

# Используем GradientTape для записи операций
with tf.GradientTape() as tape:
    y = x**2 + 3*x + 5

# Вычисляем градиент
dy_dx = tape.gradient(y, x)

print(f"f({x.numpy()}) = {y.numpy()}")  # f(2.0) = 15.0
print(f"f'({x.numpy()}) = {dy_dx.numpy()}")  # f'(2.0) = 7.0
```

### 4. Реализация простого AD (прямой режим)

```python
class Var:
    def __init__(self, value, derivative=0):
        self.value = value
        self.derivative = derivative  # производная по какому-то базовому параметру

    def __add__(self, other):
        other = other if isinstance(other, Var) else Var(other, 0)
        return Var(self.value + other.value, self.derivative + other.derivative)

    def __mul__(self, other):
        other = other if isinstance(other, Var) else Var(other, 0)
        return Var(self.value * other.value,
                  self.derivative * other.value + self.value * other.derivative)

    def __pow__(self, power):
        return Var(self.value ** power,
                  power * self.value ** (power - 1) * self.derivative)

    def __repr__(self):
        return f"Var(value={self.value}, derivative={self.derivative})"

# Пример использования
x = Var(2.0, derivative=1.0)  # производная по себе равна 1
y = x**2 + 3*x + 5

print(y)  # Var(value=15.0, derivative=7.0)
```

### 5. Вычисление градиента для функции многих переменных

```python
from autograd import grad
import autograd.numpy as np

# Функция двух переменных
def f(x, y):
    return x**2 * y + np.sin(y)

# Градиент по первому аргументу (df/dx)
df_dx = grad(f, 0)

# Градиент по второму аргументу (df/dy)
df_dy = grad(f, 1)

x, y = 2.0, 3.0
print(f"f({x}, {y}) = {f(x, y)}")  # f(2.0, 3.0) = 11.141120008
print(f"∂f/∂x = {df_dx(x, y)}")  # ∂f/∂x = 12.0
print(f"∂f/∂y = {df_dy(x, y)}")  # ∂f/∂y = 3.989992497
```

## Применение автоматического дифференцирования

1. **Оптимизация**: Градиентные методы оптимизации (градиентный спуск и его вариации)
2. **Машинное обучение**: Обратное распространение ошибки в нейронных сетях
3. **Физическое моделирование**: Чувствительность моделей к параметрам
4. **Финансы**: Вычисление "греков" в оценке производных финансовых инструментов

Автоматическое дифференцирование сочетает преимущества символьного (точность) и численного (универсальность) методов, что делает его незаменимым инструментом в современных научных и инженерных вычислениях.
